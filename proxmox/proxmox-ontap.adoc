---
sidebar: sidebar
permalink: proxmox/proxmox-ontap.html
keywords: netapp, proxmox, proxmox ve, all-flash, nfs, iscsi, ontap, storage, aff
summary: Shared storage in Proxmox Virtual Environment(VE) reduces the time for VM live migration, and makes for a better target for backups and consistent templates across the environment. ONTAP storage can serve the needs of Proxmox VE host environments as well as for guest file, block and object storage demands.
---
= Provision ONTAP storage for Proxmox Virtual Environment
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
Configure ONTAP storage with Proxmox Virtual Environment (VE) using NAS and SAN protocols. Shared storage in Proxmox VE reduces the time for live VM migration and provides a better target for backup and consistent templates across the environment. 

Proxmox VE hosts need to have FC, Ethernet, or other supported interfaces cabled to switches and have communication to ONTAP logical interfaces. Always check https://mysupport.netapp.com/matrix/#welcome[Interoperability Matrix Tool] for supported configurations.

== High-level ONTAP Features

*Common features*

* Scale out Cluster
* Secure Authentication and RBAC support
* Zero trust multi admin support
* Secure Multitenancy
* Replicate data with SnapMirror.
* Point in time copies with Snapshots.
* Space efficient clones.
* Storage efficiency features like dedupe, compression, etc.
* Trident CSI support for Kubernetes
* Snaplock
* Tamperproof Snapshot copy locking
* Encryption support
* FabricPool to tier cold data to object store.
* NetApp Console and Data Infrastructure Insights integration.
* Microsoft offloaded data transfer (ODX)

*NAS*

* FlexGroup volumes are a scale out NAS container, providing high performance along with load distribution and scalability.
* FlexCache allows data to be distributed globally and still provides local read and write access to the data.
* Multiprotocol support enables the same data to be accessible via SMB, as well as NFS.
* NFS nConnect allows multiple TCP sessions per TCP connection increasing network throughput. This increases utilization of high speed nics available on modern servers.
* NFS session trunking provides increased data transfer speeds, high availability and fault tolerance.
* SMB multichannel provides increased data transfer speed, high availability and fault tolerance.
* Integration with Active directory/LDAP for file permissions.
* Secure connection with NFS over TLS. 
* NFS Kerberos support.
* NFS over RDMA.
* Name mapping between Windows and Unix identities.
* Autonomous ransomware protection.
* File System Analytics.

*SAN*

* Stretch cluster across fault domains with SnapMirror active sync. Always check https://mysupport.netapp.com/matrix/#welcome[Interoperability Matrix Tool] for supported configurations.
* ASA models provide active/active multipathing and fast path failover.
* Support for FC, iSCSI, NVMe-oF protocols.
* Support for iSCSI CHAP mutual authentication.
* Selective LUN Map and Portset.

== Proxmox VE storage types supported with ONTAP

NAS protocols (NFS/SMB) support all content types of Proxmox VE and are typically configured once at the datacenter level. Guest VMs can use disks of type raw, qcow2, or VMDK on NAS storage.
ONTAP Snapshots can be made visible, to access point in time copies of data from the client. 
Block storage with SAN protocols (FC/iSCSI/NVMe-oF) are typically configured on a per host basis and are restricted to the VM Disk and Container Image content types supported by Proxmox VE. Guest VMs can use disks of type raw or qcow2 on block storage.

[width=100%,cols="25% 15% 15% 15% 15% 15%", frame=all, grid=all, options="header"]
|===
| Content Type | NFS | SMB/CIFS | FC | iSCSI | NVMe-oF
| Backups | Yes | Yes a| No^1^ a| No^1^ a| No^1^
| VM Disks | Yes | Yes a| Yes^2^ a| Yes^2^ a| Yes^2^
| CT Volumes | Yes | Yes a| Yes^2^ a| Yes^2^ a| Yes^2^
| ISO Images | Yes | Yes a| No^1^ a| No^1^ a| No^1^
| CT Templates | Yes | Yes a| No^1^ a| No^1^ a| No^1^
| Snippets | Yes | Yes a| No^1^ a| No^1^ a| No^1^
|===

*Notes:*
1 - Requires cluster filesystem to create the shared folder and use Directory storage type.
2 - use LVM storage type. 

== SMB/CIFS Storage 

To utilize SMB/CIFS file shares, there are certain tasks that needs to be carried out by the storage admin and the virtualization admin can mount the share using Proxmox VE UI or from shell. SMB multichannel provides fault tolerance and boosts performance. For more details, refer to link:https://www.netapp.com/pdf.html?item=/media/17136-tr4740.pdf[TR4740 - SMB 3.0 Multichannel]

NOTE: Password will be saved in clear text file and accessible only to root user. Refer to link:https://pve.proxmox.com/pve-docs/chapter-pvesm.html#storage_cifs[Proxmox VE documentation].

video::5b4ae54a-08d2-4f7d-95ec-b22d015f6035[panopto, title="SMB shared storage pool with ONTAP", width=360]

.*Storage Admin Tasks*
[%collapsible%open]
====
If new to ONTAP, use System Manager Interface to complete these tasks for a better experience.

. Ensure SVM is enabled for SMB. Follow link:https://docs.netapp.com/us-en/ontap/smb-config/configure-access-svm-task.html[ONTAP 9 documentation] for more information.

. Have at least two lifs per controller. Follow the steps from the above link. For reference, here is a screenshot of lifs used in this solution. 
+
image:proxmox-ontap-001.png[nas interface details]

. Use Active Directory or workgroup based authentication. Follow the steps from the above link.
+
image:proxmox-ontap-002.png[Join domain info]

. Create a volume. Remember to check the option to distribute data across the cluster to use FlexGroup. Make sure Anti-Ransomware protection is enabled on the volume.
+
image:proxmox-ontap-023.png[FlexGroup option]

. Create an SMB share and adjust permissions. Follow link:https://docs.netapp.com/us-en/ontap/smb-config/configure-client-access-shared-storage-concept.html[ONTAP 9 documentation] for more information.
+
image:proxmox-ontap-003.png[SMB share info]

. Provide the SMB server, Share name and credential to the virtualization admin for them to complete the task.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Collect the SMB server, share name and credentials to use for the share authentication.

. Ensure at least two interface are configured in different VLANs (for fault tolerance) and NIC supports RSS.

. If using Management UI `https:<proxmox-node>:8006`, click on datacenter, select storage, click Add and select SMB/CIFS.
+
image:proxmox-ontap-004.png[SMB storage navigation]

. Fill in the details and the share name should auto populate. Ensure all content is selected. Click Add.
+
image:proxmox-ontap-005.png[SMB storage addition]

. To enable multichannel option, go to shell on any one of the nodes on the cluster and type `pvesm set <storage id> --options multichannel,max_channels=16`
where <storage id> is the storage id created in the above step.
+
image:proxmox-ontap-006.png[multichannel setup]

. Here is the content in /etc/pve/storage.cfg for the above tasks.
+
image:proxmox-ontap-007.png[storage configuration file for SMB]
====

== NFS Storage

ONTAP supports all the NFS versions supported by Proxmox VE. To provide fault tolerance and performance enhancements, ensure link:https://docs.netapp.com/us-en/ontap/nfs-trunking/index.html[session trunking] is utilized. To use session trunking, minimum NFS v4.1 is required.

If new to ONTAP, use System Manager Interface to complete these tasks for a better experience.

video::f6c9aba3-b070-45d6-8048-b22e001acfd4[panopto, title="NFS nconnect option with ONTAP", width=360]

.*Storage Admin Tasks*
[%collapsible%open]
====
. Ensure SVM is enabled for NFS. Refer to link:https://docs.netapp.com/us-en/ontap/nfs-config/verify-protocol-enabled-svm-task.html[ONTAP 9 documentation]

. Have at least two lifs per controller. Follow the steps from the above link. For reference, here is the screenshot of lifs that we use in our lab. 
+
image:proxmox-ontap-001.png[nas interface details]

. Create or update NFS export policy providing access to Proxmox VE host IP addresses or subnet. Refer to link:https://docs.netapp.com/us-en/ontap/nfs-config/create-export-policy-task.html[Export policy creation] and link:https://docs.netapp.com/us-en/ontap/nfs-config/add-rule-export-policy-task.html[Add rule to an export policy].

. link:https://docs.netapp.com/us-en/ontap/nfs-config/create-volume-task.html[Create a volume]. For large capacity needs (>100TB), remember to check the option to distribute data across the cluster to use FlexGroup. If using FlexGroup, consider using pNFS enabled on SVM for better performance. pNFS can be enabled on SVM by following link:https://docs.netapp.com/us-en/ontap/pnfs/pnfs-commands.html#enable-nfsv4-1[Enable pNFS on SVM]. Also, make sure Anti-Ransomware protection is enabled on the volume.
+
image:proxmox-ontap-023.png[FlexGroup option]

. link:https://docs.netapp.com/us-en/ontap/nfs-config/associate-export-policy-flexvol-task.html[Assign export policy to volume]
+
image:proxmox-ontap-008.png[NFS volume info]

. Notify virtualization admin that NFS volume is ready.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Ensure at least two interface is configured in different VLANs (for fault tolerance). Use NIC bonding.

. If using Management UI `https:<proxmox-node>:8006`, click on datacenter, select storage, click Add and select NFS.
+
image:proxmox-ontap-009.png[NFS storage navigation]

. Fill in the details, After providing the server info, the NFS exports should populate and pick from the list. Remember to select the content options.
+
image:proxmox-ontap-010.png[NFS storage addition]

. To enable nConnect option, go to shell on any one of the nodes on the cluster and type `pvesm set <storage id> --options nconnect=4` 
where <storage id> is the storage id created in the above step. Note that if you plan to use session trunking feature, ensure at least NFS v4.1 is used and set the option trunkdiscovery and max_connect. `pvesm set <storage id> --options vers=4.1,trunkdiscovery,max_connect=16,nconnect=4`

. Here is the content in /etc/pve/storage.cfg for the above tasks.
+
image:proxmox-ontap-011.png[storage configuration file for NFS]

. To verify nConnect option is set, use the command `ss -an | grep :2049` on any Proxmox VE host and check for multiple connections to the NFS server IP. To verify the pNFS option is used, use the command `nfsstat -c` and check the layout related metrics. Also, based on data traffic, multiple connections to data LIFs should be visible.


NOTE: In session trunking, the nconnect option is set on only one of the trunk interfaces. With pNFS, the nconnect option is set on metadata and data interfaces.

====

== LVM with FC

//video::d66ef67f-bcc2-4ced-848e-b22e01588e8c[panopto, title="LVM shared pool with FC using ONTAP", width=360]

To configure Logical Volume Manager for shared storage across Proxmox hosts, complete for the following tasks:

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Make sure two HBA interfaces are available.

. Ensure multipath-tools is installed on all Proxmox VE hosts. Ensure it starts on boot.
+
[source,shell]
----
apt list | grep multipath-tools
# If need to install, execute the following line.
apt-get install multipath-tools
systemctl enable multipathd
----

. Collect the WWPN for all Proxmox VE hosts and provide that to the Storage admin.
+
[source,shell]
----
cat /sys/class/fc_host/host*/port_name
----
====

.*Storage Admin Tasks*
[%collapsible%open]
====
If new to ONTAP, use System Manager for a better experience.

. Ensure SVM is available with FC protocol enabled. Follow link:https://docs.netapp.com/us-en/ontap/san-admin/provision-storage.html[ONTAP 9 documentation]

. Have two lifs per controller dedicated for FC.
+
image:proxmox-ontap-024.png[FC interface details]

. Create igroup and populate the host fc initiators.

. Create the LUN with desired size on the SVM and present to igroup created in above step. Ensure Anti-Ransomware protection is enabled on the security tab on ASA systems and security tab on the volume for AFF/FAS systems.
+
image:proxmox-ontap-025.png[lun details]

. Notify virtualization admin that lun is created.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Navigate to shell on each Proxmox VE hosts in the cluster and ensure the disk is visible.
+
[source,shell]
----
lsblk -S
rescan-scsi-bus.sh
lsblk -S
---- 


. Make sure the device shows up on multipath list.
+
[source,shell]
----
multipath -ll
multipath -a /dev/sdX  # replace sdX with the device name
multipath -r
multipath -ll
----

. Create volume group 
+
[source,shell]
----
vgcreate <volume group name> /dev/mapper/<device id>
# Where <volume group name> is the desired name for the volume group and <device id> is the multipath device id.
pvs
# Verify the physical volume is part of the volume group.
vgs
# Verify the volume group is created.
----
. Go to Management UI `https:<proxmox node>:8006`, click on datacenter, select storage, click Add and select LVM.
+
image:proxmox-ontap-026.png[lvm storage navigation]

. Provide storage id name, choose existing volume group and pick the volume group that just created with cli. Remember to check the shared option. With Proxmox VE 9 and above, Check `Allow Snapshots as Volume-Chain` option which is visible when Advanced check box is enabled.
+
image:proxmox-ontap-027.png[lvm storage creation]

. Here is the sample storage configuration file for LVM using FC volume.
+
image:proxmox-ontap-028.png[lvm fc configuration]

With Proxmox VE 9 and above, the storage configuration file will have additional option `snapshot-as-volume-chain 1` when `Allow Snapshots as Volume-Chain` option is enabled.
====

== LVM with iSCSI

video::d66ef67f-bcc2-4ced-848e-b22e01588e8c[panopto, title="LVM shared pool with iSCSI using ONTAP", width=360]

To configure Logical Volume Manager for shared storage across Proxmox hosts, complete for the following tasks:

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Make sure two linux vlan interfaces are available.

. Ensure multipath-tools is installed on all Proxmox VE hosts. Ensure it starts on boot.
+
[source,shell]
----
apt list | grep multipath-tools
# If need to install, execute the following line.
apt-get install multipath-tools
systemctl enable multipathd
----

. Collect the iscsi host iqn for all Proxmox VE hosts and provide that to the Storage admin.
+
[source,shell]
----
cat /etc/iscsi/initiator.name
----
====

.*Storage Admin Tasks*
[%collapsible%open]
====
If new to ONTAP, use System Manager for a better experience.

. Ensure SVM is available with iSCSI protocol enabled. Follow link:https://docs.netapp.com/us-en/ontap/san-admin/provision-storage.html[ONTAP 9 documentation]

. Have two lifs per controller dedicated for iSCSI.
+
image:proxmox-ontap-013.png[iscsi interface details]

. Create igroup and populate the host iscsi initiators.

. Create the LUN with desired size on the SVM and present to igroup created in above step. Ensure Anti-Ransomware protection is enabled on the security tab on ASA systems. For AFF/FAS systems, ensure Anti-Ransomware protection is enabled on the security tab on the volume.
+
image:proxmox-ontap-014.png[iscsi lun details]

. Notify virtualization admin that lun is created.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Go to Management UI `https:<proxmox node>:8006`, click on datacenter, select storage, click Add and select iSCSI.
+
image:proxmox-ontap-015.png[iscsi storage navigation]

. Provide storage id name. The iSCSI lif address from ONTAP should be able to pick the target when there is no communication issue. As our intention is to not directly provide LUN access to the guest vm, uncheck that.
+
image:proxmox-ontap-016.png[iscsi storage type creation]

. Now, click Add and select LVM.
+
image:proxmox-ontap-017.png[lvm storage navigation]

. Provide storage id name, pick base storage that should match the iSCSI storage the we created in the above step. Pick the LUN for the base volume. Provide the volume group name. Ensure shared is selected. With Proxmox VE 9 and above, Check `Allow Snapshots as Volume-Chain` option which is visible when Advanced check box is enabled.
+
image:proxmox-ontap-018.png[lvm storage creation]

. Here is the sample storage configuration file for LVM using iSCSI volume.
+
image:proxmox-ontap-019.png[lvm iscsi configuration]

With Proxmox VE 9 and above, the storage configuration file will have additional option `snapshot-as-volume-chain 1` when `Allow Snapshots as Volume-Chain` option is enabled.
====
== LVM with NVMe/FC

//video::80164fe4-06db-4c21-a25d-b22e0179c3d2[panopto, title="LVM shared pool with NVMe/FC using ONTAP", width=360]

To configure Logical Volume Manager for shared storage across Proxmox hosts, complete the following tasks:

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Make sure two HBA interfaces are available.

. On every Proxmox host on the cluster, execute the following command to collect the WWPN info. Also make sure nvme-cli package is installed.
+
[source,shell]
----
apt update
apt install nvme-cli
cat /sys/class/fc_host/host*/port_name
nvme show-hostnqn
----

. Provide collected host nqn and wwpn info to storage admin and request an nvme namespace of required size.
====

.*Storage Admin Tasks*
[%collapsible%open]
====
If new to ONTAP, use System Manager for better experience.

. Ensure SVM is available with NVMe protocol enabled. Refer link:https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html[NVMe tasks on ONTAP 9 documentation].

. Create the NVMe namespace.
+
image:proxmox-ontap-029.png[nvme namespace creation]

. Create subsystem and assign host nqns (if using CLI). Follow the above reference link.

. Ensure Anti-Ransomware protection is enabled on the security tab.
+
image:proxmox-ontap-032.png[Anti-Ransomware status]

. Notify virtualization admin that the nvme namespace is created.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Navigate to shell on each Proxmox VE hosts in the cluster and verify the new namespace is visible.

. Check namespace details.
+
[source,shell]
----
nvme list
----

. Inspect and collect device details.
+
[source,shell]
----
nvme list
nvme netapp ontapdevices
nvme list-subsys
lsblk -N
----

. Create volume group 
+
[source,shell]
----
vgcreate <volume group name> /dev/mapper/<device id>
# Where <volume group name> is the desired name for the volume group and <device id> is the nvme device id.
pvs
# Verify the physical volume is part of the volume group.
vgs
# Verify the volume group is created.
----

. Go to Management UI `https:<proxmox node>:8006`, click on datacenter, select storage, click Add and select LVM.
+
image:proxmox-ontap-026.png[lvm storage navigation]

. Provide storage id name, choose existing volume group and pick the volume group that just created with cli. Remember to check the shared option. With Proxmox VE 9 and above, Check `Allow Snapshots as Volume-Chain` option which is visible when Advanced check box is enabled.
+
image:proxmox-ontap-030.png[lvm on existing vg]

. Here is a sample storage configuration file for LVM using NVMe/FC.
+
image:proxmox-ontap-031.png[lvm on nvme fc configuration]


====


== LVM with NVMe/TCP

video::80164fe4-06db-4c21-a25d-b22e0179c3d2[panopto, title="LVM shared pool with NVMe/TCP using ONTAP", width=360]

To configure Logical Volume Manager for shared storage across Proxmox hosts, complete the following tasks:

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Make sure two linux vlan interfaces are available.

. On every Proxmox host on the cluster, execute the following command to collect the host initiator info.
+
[source,shell]
----
nvme show-hostnqn
----

. Provide collected host nqn info to storage admin and request an nvme namespace of required size.
====

.*Storage Admin Tasks*
[%collapsible%open]
====
If new to ONTAP, use System Manager for better experience.

. Ensure SVM is available with NVMe protocol enabled. Refer link:https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html[NVMe tasks on ONTAP 9 documentation].

. Create the NVMe namespace.
+
image:proxmox-ontap-020.png[nvme namespace creation]

. Create subsystem and assign host nqns (if using CLI). Follow the above reference link.

. Ensure Anti-Ransomware protection is enabled on the security tab.

. Notify virtualization admin that the nvme namespace is created.
====

.*Virtualization Admin Tasks*
[%collapsible%open]
====
. Navigate to shell on each Proxmox VE hosts in the cluster and create /etc/nvme/discovery.conf file and update the content specific to your environment.
+
[source,shell]
----
root@pxmox01:~# cat /etc/nvme/discovery.conf 
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>

-t tcp -l 1800 -a 172.21.118.153
-t tcp -l 1800 -a 172.21.118.154
-t tcp -l 1800 -a 172.21.119.153
-t tcp -l 1800 -a 172.21.119.154
----

. Login to nvme subsystem
+
[source,shell]
----
nvme connect-all
----

. Inspect and collect device details.
+
[source,shell]
----
nvme list
nvme netapp ontapdevices
nvme list-subsys
lsblk -l
----

. Create volume group 
+
[source,shell]
----
vgcreate pvens02 /dev/mapper/<device id>
----

. Go to Management UI `https:<proxmox node>:8006`, click on datacenter, select storage, click Add and select LVM.
+
image:proxmox-ontap-017.png[lvm storage navigation]

. Provide storage id name, choose existing volume group and pick the volume group that just created with cli. Remember to check the shared option. With Proxmox VE 9 and above, Check `Allow Snapshots as Volume-Chain` option which is visible when Advanced check box is enabled.
+
image:proxmox-ontap-021.png[lvm on existing vg]

. Here is a sample storage configuration file for LVM using NVMe/TCP
+
image:proxmox-ontap-022.png[lvm on nvme tcp configuration]

With Proxmox VE 9 and above, the storage configuration file will have additional option `snapshot-as-volume-chain 1` when `Allow Snapshots as Volume-Chain` option is enabled. 

NOTE: nvme-cli package includes nvmef-autoconnect.service which can be enabled to automatically connect to targets on boot. Refer to nvme-cli documentation for more details.

====

